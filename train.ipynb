{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Model with Mediapipe Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import linear, relu, sigmoid\n",
    "\n",
    "\n",
    "# https://github.com/cvzone/cvzone\n",
    "from cvzone.HandTrackingModule import HandDetector      # For mediapipe hand-tracking module\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images for training\n",
    "images, labels = load_data(path='./Miniset/', grayscale=False, labels=['B'], shape=(128, 128))\n",
    "\n",
    "show_random_dataset(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "\n",
    "image = images[0].copy()\n",
    "\n",
    "hands, _ = detector.findHands(image)\n",
    "print(hands)\n",
    "\n",
    "flattened_array = []\n",
    "for landmark in hands[0].values():\n",
    "    flattened_array.extend(landmark)\n",
    "\n",
    "print(flattened_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "\n",
    "dual_hands = ('A', 'B', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'M',\n",
    "              'N', 'P', 'Q', 'R', 'S', 'T', 'W', 'X', 'Y', 'Z')\n",
    "\n",
    "def extract_from_mp_hands(mp_hands):\n",
    "\n",
    "    lmlist = mp_hands['lmList']\n",
    "    center = mp_hands['center']\n",
    "    bbox   = mp_hands['bbox']\n",
    "    _, _, W, H = bbox\n",
    "\n",
    "    data = tuple((x - center[0], y - center[1]) for x, y, _ in lmlist)\n",
    "\n",
    "    min_x = min(tuple(x for x, _ in data))\n",
    "    min_y = min(tuple(y for _, y in data))\n",
    "\n",
    "    data = tuple((x - min_x, y - min_y) for x, y in data)\n",
    "\n",
    "    # Note: Instead of dividing x by w and y by H, an alterative\n",
    "    #       approach could be divide x and y by sqrt(w*w + h*h) [TO TRY]\n",
    "    data = tuple((round((x / W), 3) , round((y / H), 3)) for x, y in data)\n",
    "\n",
    "    return tuple(val for pair in data for val in pair)\n",
    "\n",
    "\n",
    "def extract_multihands_data_with_mediapipe(frame, label):\n",
    "\n",
    "    hands, _ = detector.findHands(frame)\n",
    "\n",
    "    if len(hands) == 0 or (len(hands) == 1 and (label in dual_hands)):\n",
    "        return None\n",
    "\n",
    "    hand_type = hands[0]['type']\n",
    "    \n",
    "    part1 = extract_from_mp_hands(hands[0])\n",
    "    part2 = extract_from_mp_hands(hands[1]) if (len(hands) > 1) else (0, ) * 42\n",
    "\n",
    "    return part1 + part2 if hand_type == 'Left' else part2 + part1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe hand detector for detecting hands from a frame\n",
    "detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "\n",
    "\n",
    "# Returns medipipe_hands data from a single frame\n",
    "# Output will be a tuple of 42 values\n",
    "def extract_data_with_mediapipe(frame):\n",
    "\n",
    "    hands, _ = detector.findHands(frame)\n",
    "\n",
    "    if len(hands) == 0:\n",
    "        return None\n",
    "\n",
    "    lmlist = hands[0]['lmList']\n",
    "    center = hands[0]['center']\n",
    "    bbox = hands[0]['bbox']\n",
    "    _, _, W, H = bbox\n",
    "\n",
    "    data = tuple((x - center[0], y - center[1]) for x, y, _ in lmlist)\n",
    "\n",
    "    min_x = min(tuple(x for x, _ in data))\n",
    "    min_y = min(tuple(y for _, y in data))\n",
    "\n",
    "    data = tuple((x - min_x, y - min_y) for x, y in data)\n",
    "\n",
    "    # Note: Instead of dividing x by w and y by H, an alterative\n",
    "    #       approach could be divide x and y by sqrt(w*w + h*h) [TO TRY]\n",
    "    data = tuple((round((x / W), 3) , round((y / H), 3)) for x, y in data)\n",
    "\n",
    "    return tuple(val for pair in data for val in pair)\n",
    "\n",
    "\n",
    "# Generate mediapipe_hands data over entire image set\n",
    "# using extract_data_with_mediapipe function\n",
    "# Returns an np array of shape(no. of images, 42)\n",
    "def generate_mediapipe_data(path, labels=None, shape=(128, 128)):\n",
    "\n",
    "    # Get a list of all the folders in the directory\n",
    "    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "\n",
    "    if labels is not None:\n",
    "        folders = [f for f in folders if f in labels]\n",
    "\n",
    "    folders.sort()\n",
    "\n",
    "    # Print the list of folders\n",
    "    print(\"Folders_Found = \", folders)\n",
    "\n",
    "    for folder in folders:\n",
    "        # Get a list of all the image files in the directory (supported file extensions: .jpg, .jpeg, .png, .bmp, .gif, .tiff)\n",
    "        images_path = glob.glob(os.path.join(path + \"/\" + folder, '*.jpg'))\n",
    "\n",
    "\n",
    "        # By default, the color space of the loaded image is BGR (blue, green, red)\n",
    "        # rather than the typical RGB (red, green, blue) order.\n",
    "        images = tuple(cv2.imread(image_path) for image_path in images_path)\n",
    "\n",
    "        # Resizing images to reduce computational load.\n",
    "        images = tuple(cv2.resize(image, shape) for image in images)\n",
    "\n",
    "\n",
    "        images_mediapipe = ()\n",
    "        passed = 0\n",
    "\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            # mp_data = extract_data_with_mediapipe(image.copy())\n",
    "            mp_data = extract_multihands_data_with_mediapipe(image, folder)\n",
    "\n",
    "            if mp_data is None:\n",
    "                print(f'Empty hands at : Folder {folder} | {images_path[i]}')\n",
    "            else:\n",
    "                passed += 1\n",
    "                images_mediapipe += (mp_data, )\n",
    "\n",
    "        # images_mediapipe2 = tuple(extract_data_with_mediapipe(image.copy()) for image in images)\n",
    "\n",
    "        labels = tuple(folder * passed)\n",
    "        combined_data = np.hstack((images_mediapipe, np.array(labels)[:, np.newaxis]))\n",
    "\n",
    "        file_name = 'mp_data_isl_' + folder + '.csv'\n",
    "        np.savetxt(file_name, combined_data, delimiter=',', fmt='%s')\n",
    "\n",
    "        print(f'{passed}/{len(images)} images extracted to .csv from folder {folder}')\n",
    "\n",
    "\n",
    "# generate_mediapipe_data('C:/Users/subha/Downloads/archive/ASL_Dataset/Train/',\n",
    "#                         labels=['M', 'N', 'O'])\n",
    "\n",
    "generate_mediapipe_data('./Dataset/Indian/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training/test data from .csv files to memory\n",
    "\n",
    "# Set the directory path where the CSV files are located\n",
    "directory_path = './mediapipe_dataset/'\n",
    "\n",
    "# Initialize an empty list to store the loaded data\n",
    "images, labels = [], []\n",
    "\n",
    "\n",
    "for letter in range(ord('A'), ord('Z')+1):\n",
    "    # Create the CSV file path for the current letter\n",
    "    csv_file_path = os.path.join(directory_path, f'mediapipe_data_{chr(letter)}.csv')\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "    # Extract the image data and labels from the DataFrame\n",
    "    images.append(df.iloc[:, :-1].values)\n",
    "    labels.append(df.iloc[:, -1].values)\n",
    "\n",
    "images = np.concatenate(images, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split    # For splitting data into train and test sets.\n",
    "\n",
    "\n",
    "# Convert labels to integers using label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into training and test sets (using 80-20 split)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(images, encoded_labels, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model architecture\n",
    "\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(42, )),  # To be adjusted for inputs\n",
    "        Dense(39, activation='relu'),\n",
    "        Dense(36, activation='linear')\n",
    "\n",
    "    ], name = \"sign_recognition_model\" \n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    validation_data=(test_data, test_labels),\n",
    "    epochs=20,\n",
    ")\n",
    "\n",
    "model.save('isl_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy, val_accuracy, loss, val_loss\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot the validation loss\n",
    "ax1.plot(history.history['loss'], label='loss')\n",
    "ax1.plot(history.history['val_loss'], label='val_loss')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "predictions = model.predict(test_data)\n",
    "predictions_categorical = np.argmax(predictions, axis=1)\n",
    "confusion_mtx = confusion_matrix(test_labels, predictions_categorical)\n",
    "\n",
    "class_names = [chr(i + 65) for i in range(26)]      # 'A' to 'Z'\n",
    "\n",
    "# Plot the confusion matrix using seaborn heatmap\n",
    "sns.heatmap(confusion_mtx, annot=True,\n",
    "            cmap=plt.cm.Blues, fmt='g',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "print(confusion_mtx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming you have your test data and labels stored in variables test_data and test_labels\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "\n",
    "# Convert predictions to categorical format if needed\n",
    "predictions_categorical = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(test_labels, predictions_categorical, average='weighted')\n",
    "\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert predictions to categorical format if needed\n",
    "predictions_categorical = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_labels, predictions_categorical)\n",
    "\n",
    "# Print the report\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
