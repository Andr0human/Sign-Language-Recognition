{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Model with Mediapipe Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import linear, relu, sigmoid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cvzone.HandTrackingModule import HandDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe hand detector for detecting hands from a frame\n",
    "detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "\n",
    "# Returns medipipe_hands data from a single frame\n",
    "# Output will be a tuple of 42 values\n",
    "def extract_data_with_mediapipe(frame):\n",
    "\n",
    "    hands, _ = detector.findHands(frame)\n",
    "\n",
    "    if len(hands) == 0:\n",
    "        raise Exception('No hand found!')\n",
    "\n",
    "    lmlist = hands[0]['lmList']\n",
    "    center = hands[0]['center']\n",
    "    bbox = hands[0]['bbox']\n",
    "    _, _, W, H = bbox\n",
    "\n",
    "    data = tuple((x - center[0], y - center[1]) for x, y, _ in lmlist)\n",
    "\n",
    "    min_x = min(tuple(x for x, _ in data))\n",
    "    min_y = min(tuple(y for _, y in data))\n",
    "\n",
    "    data = tuple((x - min_x, y - min_y) for x, y in data)\n",
    "\n",
    "    # Note: Instead of dividing x by w and y by H, an alterative\n",
    "    #       approach could be divide x and y by sqrt(w*w + h*h) [TO TRY]\n",
    "    data = tuple((round((x / W), 3) , round((y / H), 3)) for x, y in data)\n",
    "\n",
    "    return tuple(val for pair in data for val in pair)\n",
    "\n",
    "\n",
    "# Generate mediapipe_hands data over entire image set\n",
    "# using extract_data_with_mediapipe function\n",
    "# Returns an np array of shape(no. of images, 42)\n",
    "def generate_mediapipe_data(path, labels=None, shape=(128, 128)):\n",
    "\n",
    "    # Get a list of all the folders in the directory\n",
    "    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "\n",
    "    if labels is not None:\n",
    "        folders = [f for f in folders if f in labels]\n",
    "\n",
    "    folders.sort()\n",
    "\n",
    "    # Print the list of folders\n",
    "    print(\"Folders_Found = \", folders)\n",
    "\n",
    "    data, labels = (), ()\n",
    "\n",
    "    for folder in folders:\n",
    "        # Get a list of all the image files in the directory (supported file extensions: .jpg, .jpeg, .png, .bmp, .gif, .tiff)\n",
    "        images_path = glob.glob(os.path.join(path + \"/\" + folder, '*.jpg'))\n",
    "\n",
    "\n",
    "        # By default, the color space of the loaded image is BGR (blue, green, red)\n",
    "        # rather than the typical RGB (red, green, blue) order.\n",
    "        images = tuple(cv2.imread(image_path) for image_path in images_path)\n",
    "\n",
    "        # Resizing images to reduce computational load.\n",
    "        images = tuple(cv2.resize(image, shape) for image in images)\n",
    "\n",
    "        # print(images[0].shape)\n",
    "\n",
    "        images_mediapipe = tuple(extract_data_with_mediapipe(image) for image in images)\n",
    "\n",
    "        data   += images_mediapipe\n",
    "        labels += tuple(folder * len(images))\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# Encoder for encoding labels from ['0' -> 0, 'A' -> 10, 'K' -> 20, 'Z' -> 35]\n",
    "def encoder(label):\n",
    "    if label <= '9':\n",
    "        return ord(label) - 48\n",
    "    return ord(label) - 55\n",
    "\n",
    "\n",
    "def encode_labels(labels):\n",
    "    t = tuple(encoder(label) for label in labels)\n",
    "    return np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders_Found =  ['1', '2', 'C']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Loading data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m mediapipe_images, labels \u001b[39m=\u001b[39m generate_mediapipe_data(\u001b[39m'\u001b[39;49m\u001b[39m./Dataset/Indian/\u001b[39;49m\u001b[39m'\u001b[39;49m, labels\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      4\u001b[0m combined_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((mediapipe_images, np\u001b[39m.\u001b[39marray(labels)[:, np\u001b[39m.\u001b[39mnewaxis]))\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(mediapipe_images\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[89], line 64\u001b[0m, in \u001b[0;36mgenerate_mediapipe_data\u001b[1;34m(path, labels, shape)\u001b[0m\n\u001b[0;32m     60\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(cv2\u001b[39m.\u001b[39mresize(image, shape) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     62\u001b[0m \u001b[39m# print(images[0].shape)\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m images_mediapipe \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(extract_data_with_mediapipe(image) \u001b[39mfor\u001b[39;49;00m image \u001b[39min\u001b[39;49;00m images)\n\u001b[0;32m     66\u001b[0m data   \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m images_mediapipe\n\u001b[0;32m     67\u001b[0m labels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(folder \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(images))\n",
      "Cell \u001b[1;32mIn[89], line 64\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(cv2\u001b[39m.\u001b[39mresize(image, shape) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     62\u001b[0m \u001b[39m# print(images[0].shape)\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m images_mediapipe \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(extract_data_with_mediapipe(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     66\u001b[0m data   \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m images_mediapipe\n\u001b[0;32m     67\u001b[0m labels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(folder \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(images))\n",
      "Cell \u001b[1;32mIn[89], line 8\u001b[0m, in \u001b[0;36mextract_data_with_mediapipe\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_data_with_mediapipe\u001b[39m(frame):\n\u001b[1;32m----> 8\u001b[0m     hands, _ \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mfindHands(frame)\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(hands) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     11\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNo hand found!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cvzone\\HandTrackingModule.py:49\u001b[0m, in \u001b[0;36mHandDetector.findHands\u001b[1;34m(self, img, draw, flipType)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mFinds hands in a BGR image.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m:param img: Image to find the hands in.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m:param draw: Flag to draw the output on the image.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m:return: Image with or without drawings\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m imgRGB \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhands\u001b[39m.\u001b[39;49mprocess(imgRGB)\n\u001b[0;32m     50\u001b[0m allHands \u001b[39m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m h, w, c \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
      "File \u001b[1;32mc:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "mediapipe_images, labels = generate_mediapipe_data('./Dataset/Indian/', labels=['1', '2', 'C'])\n",
    "combined_data = np.hstack((mediapipe_images, np.array(labels)[:, np.newaxis]))\n",
    "\n",
    "print(mediapipe_images.shape)\n",
    "\n",
    "np.savetxt('mediapipe_data.csv', combined_data, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 42)\n",
      "[ 1  1  1 ... 12 12 12]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('mediapipe_data.csv', header=None)\n",
    "\n",
    "# Extract the mediapipe images and labels as numpy arrays\n",
    "images = df.iloc[:, :-1].values\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "y_labels = encode_labels(labels)\n",
    "\n",
    "print(images.shape)\n",
    "print(y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up an Early Stopper\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\tif logs.get('val_auc') > 0.99:\n",
    "\t\t\tprint('\\n Validation accuracy has reached upto 99%\\\n",
    "\tso, stopping further training.')\n",
    "\t\t\tself.model.stop_training = True\n",
    "\n",
    "es = EarlyStopping(patience=3,\n",
    "\t\t\t\tmonitor='val_auc',\n",
    "\t\t\t\trestore_best_weights=True)\n",
    "\n",
    "lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "\t\t\t\t\tpatience=2,\n",
    "\t\t\t\t\tfactor=0.5,\n",
    "\t\t\t\t\tverbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sign_recognition_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 39)                1677      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 36)                1440      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,117\n",
      "Trainable params: 3,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up the model parameters\n",
    "\n",
    "\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(42, )),  # To be adjusted for inputs\n",
    "        Dense(39, activation='relu'),\n",
    "        Dense(36, activation='linear')\n",
    "\n",
    "    ], name = \"sign_recognition_model\" \n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "113/113 [==============================] - 2s 2ms/step - loss: 0.0028\n",
      "Epoch 2/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 7.1319e-04\n",
      "Epoch 3/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 3.3821e-04\n",
      "Epoch 4/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 1.9950e-04\n",
      "Epoch 5/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 1.3278e-04\n",
      "Epoch 6/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 9.4445e-05\n",
      "Epoch 7/20\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 7.0624e-05\n",
      "Epoch 8/20\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 5.5070e-05\n",
      "Epoch 9/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 4.4010e-05\n",
      "Epoch 10/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 3.5952e-05\n",
      "Epoch 11/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 2.9902e-05\n",
      "Epoch 12/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 2.5192e-05\n",
      "Epoch 13/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 2.1461e-05\n",
      "Epoch 14/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 1.8524e-05\n",
      "Epoch 15/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 1.6107e-05\n",
      "Epoch 16/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 1.4097e-05\n",
      "Epoch 17/20\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 1.2409e-05\n",
      "Epoch 18/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 1.1010e-05\n",
      "Epoch 19/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 9.8054e-06\n",
      "Epoch 20/20\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 8.7556e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compiling and training the model\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    images, y_labels,\n",
    "    epochs=20,\n",
    "    # callbacks=[es, lr, myCallback()]\n",
    ")\n",
    "\n",
    "model.save('isl_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
